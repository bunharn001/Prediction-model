This is project studies on Basic Data Preprocessing and easy Prediction model insist of Data Preprocessing and training prediction model with one-hot encoder (binary-class label)

Addditional for feature scaling

Feature scaling
Feature scaling marks the end of the data preprocessing in Machine Learning. It is a method to standardize the independent variables of a dataset within a specific range. In other words, feature scaling limits the range of variables so that you can compare them on common grounds.

Another reason why feature scaling is applied is that few algorithms like gradient descent converge much faster with feature scaling than without it.

<img width="876" height="533" alt="image" src="https://github.com/user-attachments/assets/034207b3-8a47-4445-8a58-f3e90b6c3b81" />

Why feature scalling? 
Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.

If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.

<img width="395" height="567" alt="image" src="https://github.com/user-attachments/assets/a5f014b5-29ff-4ae8-9a38-b70f3bc4fc77" />


credit: information and all of the code original from https://www.kaggle.com/code/alirezahasannejad/data-preprocessing-in-machine-learning/notebook
creator: Alireza Hasannejad

